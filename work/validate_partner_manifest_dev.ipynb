{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import ete3\n",
    "import geopy\n",
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_VERSION = '0.3.5dev'\n",
    "ANOSPP_VERSION = '4.0'\n",
    "# V2.0, but V3 in SOP\n",
    "BIOSCAN_VERSION = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anospp_fn = '../data/Anopheles_Metadata_Manifest_V4.0_20221220.xlsx'\n",
    "biosc_fn = '../data/BIOSCAN_Manifest_V3_20240301.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and install taxonomy\n",
    "ncbi = ete3.NCBITaxa()\n",
    "# run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fn, sheet='TAB 2 Metadata Entry'):\n",
    "\n",
    "    logging.debug(f'reading data from \"{fn}\" sheet \"{sheet}\"')\n",
    "    \n",
    "    df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                       sheet_name=sheet)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# df = get_data(anospp_fn, sheet='TAB 3 TEST Metadata Entry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bold(fn, template_df, sheet):\n",
    "    \n",
    "    \n",
    "    \n",
    "    xl = pd.ExcelFile(fn)\n",
    "\n",
    "    if sheet in xl.sheet_names: \n",
    "        logging.info(f'parsing bold manifest from \"{fn}\" sheet \"{sheet}\"')\n",
    "        # second BGE manifest header\n",
    "        df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                           sheet_name=sheet, header=1).iloc[1:]\n",
    "        if 'Plate ID' not in df.columns:\n",
    "            # first BGE manifest header\n",
    "            df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                               sheet_name=sheet, header=3).iloc[3:]\n",
    "    \n",
    "    else:\n",
    "        logging.info(f'sheet {sheet} not found in bold manifest \"{fn}\", aggregating data from available fields')\n",
    "        first_df = True\n",
    "        for sub_sheet in xl.sheet_names:\n",
    "            logging.debug(f'parsing sheet {sub_sheet}')\n",
    "            if sub_sheet == 'Custom':\n",
    "                sub_df = pd.read_excel(\n",
    "                    fn, sheet_name=sub_sheet,\n",
    "                    dtype=str, keep_default_na=False,\n",
    "                    header=3)\n",
    "                sub_df.columns = sub_df.columns.str.replace('SampleID','Sample ID')\n",
    "            else:\n",
    "                sub_df = pd.read_excel(\n",
    "                    fn, sheet_name=sub_sheet,\n",
    "                    dtype=str, keep_default_na=False,\n",
    "                    header=2)\n",
    "            if 'Sample ID' not in sub_df.columns:\n",
    "                logging.warning(f'sheet {sub_sheet} does not contain \"Samlpe ID\" column, skipping')\n",
    "                continue\n",
    "            # first entry\n",
    "            if first_df:\n",
    "                df = sub_df\n",
    "                first_df = False\n",
    "            # subsequent\n",
    "            else:\n",
    "                dup_cols = set(sub_df.columns).intersection(set(df.columns) - set(['Sample ID']))\n",
    "                if len(dup_cols) > 0:\n",
    "                    logging.warning(f'found and ignored duplicate columns in sheet {sub_sheet}: {\", \".join(dup_cols)}')\n",
    "                    sub_df = sub_df.drop(columns=dup_cols)\n",
    "                nsamples = len(df)\n",
    "                df = df.merge(sub_df, on='Sample ID', how='inner')\n",
    "                assert len(df) == nsamples, f'not all samples found in sheet {sub_sheet}'\n",
    "                      \n",
    "    logging.info('post-processing BOLD manifest')\n",
    "    \n",
    "    df = df.rename(columns={\n",
    "        'State/ Province':'State/Province',\n",
    "        'Country/ Ocean':'Country/Ocean'\n",
    "    })\n",
    "    \n",
    "    logging.debug('concatenating site info')\n",
    "    def agg_loc(row):\n",
    "        loc_info = []\n",
    "        for col in ['State/Province', 'Region', 'Sector', 'Exact Site']:\n",
    "            if len(row[col]) > 0:\n",
    "                loc_info.append(row[col])\n",
    "        return ', '.join(loc_info)\n",
    "    \n",
    "    df['Exact Site'] = df.apply(agg_loc, axis=1)\n",
    "    \n",
    "    logging.debug('remapping columns')\n",
    "    col_mapping = {\n",
    "#         'Sample ID'\n",
    "        'Plate ID':'RACK_OR_PLATE_ID',\n",
    "        'Well Position':'TUBE_OR_WELL_ID',\n",
    "        'Field ID':'COLLECTOR_SAMPLE_ID',\n",
    "#         'Museum ID'\n",
    "#         'Collection Code'\n",
    "#         'Institution Storing'\n",
    "#         'institution ID'\n",
    "#         'Additional ID'\n",
    "#         'Type of Additional ID'\n",
    "#         'Phylum'\n",
    "#         'Class'\n",
    "        'Order':'PREDICTED_ORDER_OR_GROUP',\n",
    "        'Family':'PREDICTED_FAMILY',\n",
    "#         'Subfamily'\n",
    "#         'Tribe'\n",
    "        'Genus':'PREDICTED_GENUS',\n",
    "        'Species':'PREDICTED_SCIENTIFIC_NAME',\n",
    "#         'Subspecies'\n",
    "        'Identifier':'IDENTIFIED_BY',\n",
    "#         'Identifier ORCID'\n",
    "#         'Identifier Email'\n",
    "#         'Identification Method'\n",
    "#         'Identification Date'\n",
    "#         'Taxonomy Notes'\n",
    "        'Sex':'SEX',\n",
    "#         'Reproduction'\n",
    "        'Life Stage':'LIFESTAGE',\n",
    "#         'Extra Info'\n",
    "#         'Notes'\n",
    "#         'Voucher Status'\n",
    "        'Voucher preservation':'PRESERVATIVE_SOLUTION',\n",
    "#         'Type Status'\n",
    "        'Tissue Descriptor':'ORGANISM_PART',\n",
    "#         'External URLs'\n",
    "#         'Associated Taxa'\n",
    "#         'Associated Specimens'\n",
    "#         'Collectors'\n",
    "        'Collection Date':'DATE_OF_COLLECTION',\n",
    "        'Collection Start Date':'DATE_OF_COLLECTION',\n",
    "        'Country/Ocean':'COUNTRY_OF_COLLECTION',\n",
    "#         'State/Province'\n",
    "#         'Region'\n",
    "#         'Sector'\n",
    "        'Exact Site':'COLLECTION_LOCATION',\n",
    "        'Lat':'DECIMAL_LATITUDE',\n",
    "        'Lon':'DECIMAL_LONGITUDE',\n",
    "#         'GPS Source'\n",
    "        'Elev':'ELEVATION',\n",
    "#         'Elevation Precision'\n",
    "#         'Depth'\n",
    "#         'Depth Precision'\n",
    "#         'Coordinate Accuracy'\n",
    "#         'Event Time'\n",
    "#         'Collection Date Accuracy'\n",
    "#         'Collection End Date'\n",
    "        'Habitat':'HABITAT',\n",
    "        'Sampling Protocol':'DESCRIPTION_OF_COLLECTION_METHOD',\n",
    "#         'Collection Notes'\n",
    "#         'Site Code'\n",
    "#         'Collection Event ID'\n",
    "#         'Tissue biobanked?'\n",
    "#         'Tissue Biobank ID'\n",
    "#         '2D Barcode ID Tissue'\n",
    "#         'Tissue Biobank Name'\n",
    "#         'Biobanked tissue type'\n",
    "#         'Preparation type (Tissue)'\n",
    "#         'Preservation history'\n",
    "#         'Relation To Voucher'\n",
    "#         'DNA Biobank ID'\n",
    "#         '2D Barcode ID DNA'\n",
    "#         'DNA Biobank Name'\n",
    "#         'Preparation Type (DNA)'\n",
    "#         'Extraction Method'\n",
    "#         'Extraction Date'\n",
    "#         'Extraction Staff or Lab'\n",
    "#         'DNA Concentration [ng/µL]'\n",
    "#         'Quantification Method'\n",
    "#         'Quantification Date'\n",
    "#         'DNA Volume [µL]'\n",
    "#         'GGBN Upload Mechanism'\n",
    "        'BIOSAMPLE_ID':'biosample ID',\n",
    "        'INSDC_PROJECT_ID':'INSDC project ID'\n",
    "        # 'Permits'\n",
    "    }\n",
    "    \n",
    "    df.rename(columns=col_mapping, inplace=True)\n",
    "    \n",
    "    logging.debug('reordering columns, adding missing, removing \"LEAVE BLANK\" values')\n",
    "    for col in template_df.columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "        else:\n",
    "            df[col].replace({\n",
    "                'LEAVE BLANK':'',\n",
    "                'LEFT BLANK':''\n",
    "            }, inplace=True)\n",
    "    \n",
    "    df = df[template_df.columns]\n",
    "    \n",
    "    \n",
    "    logging.debug('stripping leading zeroes from well ids')\n",
    "    df['TUBE_OR_WELL_ID'] = df['TUBE_OR_WELL_ID'].str.replace(\n",
    "        '([A-H])0', r'\\1', regex=True\n",
    "    )\n",
    "    \n",
    "    logging.debug('fixing organism part for blanks - assume no data')\n",
    "    for i, r in df[df['ORGANISM_PART'] == ''].iterrows():\n",
    "        df.loc[i, 'ORGANISM_PART'] = 'NOT_APPLICABLE'\n",
    "        if df.loc[i, 'TUBE_OR_WELL_ID'] == '':\n",
    "            # originally we wanted to backfill well IDs, \n",
    "            # but BGEP has varying order of wells in the manifest\n",
    "            # thus not handling for now\n",
    "            raise ValueError(f'Cannot handle empty well ids for blanks, row {i+1} - please fill')\n",
    "        elif r['TUBE_OR_WELL_ID'] == 'A1':\n",
    "            raise ValueError(f'Cannot handle blanks at A1, row {i+1}')\n",
    "        df.loc[i, 'RACK_OR_PLATE_ID'] = df.loc[i-1, 'RACK_OR_PLATE_ID']\n",
    "        df.loc[i, 'PRESERVATIVE_SOLUTION'] = df.loc[i-1, 'PRESERVATIVE_SOLUTION']\n",
    "\n",
    "    # sts manifest is ordered as A1,B1\n",
    "    sts_well_ids = []\n",
    "    for col in range(1,13):\n",
    "        for row in 'ABCDEFGH':\n",
    "            sts_well_ids.append(f'{row}{col}')\n",
    "    logging.debug('adding missing wells - assume blank samples')\n",
    "    # assume missing wells are blanks, add at the end, sort later\n",
    "    for plate_id in df.RACK_OR_PLATE_ID.drop_duplicates():\n",
    "        plate_df = df[df['RACK_OR_PLATE_ID'] == plate_id]\n",
    "        missing_wells = set(sts_well_ids) - set(plate_df['TUBE_OR_WELL_ID'])\n",
    "        if len(missing_wells) > 0:\n",
    "            logging.warning(\n",
    "                f'plate {plate_id} has missing wells {\", \".join(missing_wells)}, '\n",
    "                f'filling with blank samples'\n",
    "            )\n",
    "            md = {}\n",
    "            for i, well_id in enumerate(missing_wells):\n",
    "                md[i] = {\n",
    "                    'RACK_OR_PLATE_ID':plate_id,\n",
    "                    'TUBE_OR_WELL_ID':well_id,\n",
    "                    'PRESERVATIVE_SOLUTION':plate_df['PRESERVATIVE_SOLUTION'].iloc[0],\n",
    "                    'ORGANISM_PART':'NOT_APPLICABLE'\n",
    "                }\n",
    "            df = pd.concat([df, pd.DataFrame(md).T], ignore_index=True).fillna('')\n",
    "                \n",
    "    logging.debug('sorting by plate and well')\n",
    "    well_order_cat = pd.CategoricalDtype(sts_well_ids, ordered=True)\n",
    "    df['TUBE_OR_WELL_ID'] = df['TUBE_OR_WELL_ID'].astype(well_order_cat)\n",
    "    plate_order_cat = pd.CategoricalDtype(df.RACK_OR_PLATE_ID.drop_duplicates(), ordered=True)\n",
    "    df['RACK_OR_PLATE_ID'] = df['RACK_OR_PLATE_ID'].astype(plate_order_cat)\n",
    "    df = df.sort_values(by=['RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID'])\n",
    "    for col in ['RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID']:\n",
    "        df[col] = df[col].astype(str)\n",
    "    # assign series from new index\n",
    "    df.index = (i + 1 for i in range(df.shape[0]))\n",
    "    df.index.name = 'SERIES'\n",
    "    \n",
    "    logging.debug('adjusting specific content')\n",
    "    df['SEX'] = df['SEX'].str.upper()\n",
    "    df['LIFESTAGE'] = df['LIFESTAGE'].str.upper()\n",
    "    df['LIFESTAGE'] = df['LIFESTAGE'].str.replace('LARVA.*','LARVA', regex=True)\n",
    "    df['LIFESTAGE'].replace({\n",
    "        'IMMATURE / CATERPILLAR':'LARVA',\n",
    "        'NYMPH':'LARVA',\n",
    "        'NYMPHS':'LARVA',\n",
    "        'SUBADULT':'ADULT',\n",
    "        'SUB-ADULT':'ADULT'\n",
    "    }, inplace=True)\n",
    "    df['ORGANISM_PART'] = df['ORGANISM_PART'].str.upper() \\\n",
    "        .str.replace(', ', ' | ') \\\n",
    "        .str.replace(' AND ', ' | ') \\\n",
    "        .str.replace('PARTIAL ', '') \\\n",
    "        .str.strip()\n",
    "    df['ORGANISM_PART'].replace({\n",
    "        'LEGS':'LEG',\n",
    "        'CLAW':'LEG',\n",
    "        'LEG (FEMUR)':'LEG',\n",
    "        'ABDOMMEN':'ABDOMEN',\n",
    "        'WINGS':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'WING':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'ABDOMEN | LEGS':'ABDOMEN | LEG',\n",
    "        'HEAD | LEGS':'HEAD | LEG',\n",
    "        'LEGS | THORAX':'LEG | THORAX',\n",
    "        'HEAD THORAX':'HEAD | THORAX',\n",
    "        'HEAD_THORAX':'HEAD | THORAX',\n",
    "        'THORAX HEAD':'HEAD | THORAX',\n",
    "        'ABDOMEN THORAX':'ABDOMEN | THORAX',\n",
    "        'GASTER':'ABDOMEN',\n",
    "        'BODY':'WHOLE_ORGANISM',\n",
    "        'WHOLE':'WHOLE_ORGANISM',\n",
    "        'TOTAL':'WHOLE_ORGANISM',\n",
    "        'WHOLE BODY':'WHOLE_ORGANISM',\n",
    "        'WHOLE ORGANISM':'WHOLE_ORGANISM',\n",
    "        'ORGANISM':'WHOLE_ORGANISM',\n",
    "        'WHOLE SPECIMEN':'WHOLE_ORGANISM',\n",
    "        'WHOLE_SPECIMEN':'WHOLE_ORGANISM',\n",
    "        'MUSCLE':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'BODY PARTS':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'BODY PART':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'SOMATIC PART':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'TAIL':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'ANTENA':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'ANTENNA':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'HALF_SPECIMEN':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'PIECE OF BODY':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "        'MUSCLE TISSUE':'**OTHER_SOMATIC_ANIMAL_TISSUE**',\n",
    "    }, inplace=True)\n",
    "    df['PRESERVATIVE_SOLUTION'].replace({\n",
    "        'Ethanol':'V%_ETHANOL',\n",
    "        'ethanol':'V%_ETHANOL',\n",
    "        'dry':'NONE',\n",
    "        'Dry':'NONE',\n",
    "        '':'NONE' # problem with blanks?\n",
    "    }, inplace=True)\n",
    "    df['CATCH_SOLUTION'].replace({\n",
    "        'Ethanol':'V%_ETHANOL',\n",
    "        'ethanol':'V%_ETHANOL',\n",
    "        'dry':'NONE',\n",
    "        'Dry':'NONE',\n",
    "        '':'NONE' # problem with blanks?\n",
    "    }, inplace=True)\n",
    "    df['DECIMAL_LATITUDE'] = df['DECIMAL_LATITUDE'].replace('', np.nan).astype(float).round(6).astype(str)\n",
    "    df['DECIMAL_LONGITUDE'] = df['DECIMAL_LONGITUDE'].replace('', np.nan).astype(float).round(6).astype(str)\n",
    "    # df['DATE_OF_COLLECTION'] = df['DATE_OF_COLLECTION'].str.split('/').str.get(-1)\n",
    "    df['DATE_OF_COLLECTION'] = pd.to_datetime(df['DATE_OF_COLLECTION'], errors='coerce') \\\n",
    "            .dt.strftime('%Y-%m-%d') \\\n",
    "            .fillna('')\n",
    "    species_identified = (df['PREDICTED_SCIENTIFIC_NAME'].str.contains('[a-z]', regex=True))\n",
    "    df.loc[species_identified, 'PREDICTED_SCIENTIFIC_NAME'] = df.loc[species_identified, 'PREDICTED_GENUS'].str.title() + \\\n",
    "            ' ' + df.loc[species_identified, 'PREDICTED_SCIENTIFIC_NAME'].str.lower()\n",
    "        \n",
    "    logging.debug('auto-filling content not in BOLD manifest')\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    df['CATCH_LOT'] = 'NOT_APPLICABLE'\n",
    "    df.loc[~is_blank, 'CATCH_SOLUTION'] = df['PRESERVATIVE_SOLUTION']\n",
    "    df.loc[is_blank, 'CATCH_SOLUTION'] = ''\n",
    "    df.loc[~is_blank, 'BOTTLE_DIRECTION'] = ''\n",
    "    df.loc[~is_blank, 'AMOUNT_OF_CATCH_PLATED'] = 'NOT_APPLICABLE'\n",
    "    \n",
    "    # encoding\n",
    "    \n",
    "    logging.debug('encoding collection methods')\n",
    "    df.loc[df['DESCRIPTION_OF_COLLECTION_METHOD'].str.lower() \\\n",
    "           .str.contains('pan trap'), 'COLLECTION_METHOD'] = 'PAN_TRAP'\n",
    "    logging.info(f'inferred {df[df.COLLECTION_METHOD == \"PAN_TRAP\"].shape[0]} samples collected with PAN_TRAP')\n",
    "    df.loc[df['DESCRIPTION_OF_COLLECTION_METHOD'].str.lower() \\\n",
    "           .str.contains('malaise trap'), 'COLLECTION_METHOD'] = 'MALAISE_TRAP'\n",
    "    logging.info(f'inferred {df[df.COLLECTION_METHOD == \"MALAISE_TRAP\"].shape[0]} samples collected with MALAISE_TRAP')\n",
    "    df.loc[df['DESCRIPTION_OF_COLLECTION_METHOD'].str.lower() \\\n",
    "           .str.contains('sweep net'), 'COLLECTION_METHOD'] = 'AERIAL_NET'\n",
    "    df.loc[~is_blank & (df.COLLECTION_METHOD == ''), 'COLLECTION_METHOD'] = '**OTHER**'\n",
    "    \n",
    "    logging.info('post-processing BOLD manifest complete')\n",
    "    \n",
    "    return df\n",
    "# template_df = get_data(biosc_fn, sheet='TAB 2 Metadata Entry')\n",
    "# # bold_fn = '../results/20231006_bold_test/BGE_single_specimen_metadata&biobanking_sheet_v5_Kaliuzhna_Braconidae.xlsx'\n",
    "# bold_fn = '../results/20240730_bold_bge_poland/BGEP_am60.xlsx'\n",
    "# df = parse_bold(bold_fn, template_df, 'BGE entry')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_date_formats(df):\n",
    "    # time can be appended to date by conversion to string in pd.read_excel\n",
    "    # fixing date formats appearing due to string conversion on reading\n",
    "    logging.debug('fixing date formats by removing appended times')\n",
    "    \n",
    "    for col in ['DATE_OF_COLLECTION', 'DATE_OF_PLATING', 'DATE_OF_PRESERVATION']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.replace(r' 00:00:00$','')\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_bioscan_version(df):\n",
    "    \n",
    "    logging.debug(f'inferring bioscan version from data format')\n",
    "    \n",
    "    version_evidence = {}\n",
    "    \n",
    "    # columns in v3, not in v2\n",
    "    for col in ('CATCH_SOLUTION', 'AMOUNT_OF_CATCH_PLATED'):\n",
    "        if col in df.columns:\n",
    "            version_evidence[col] = 'v3'\n",
    "        else:\n",
    "            version_evidence[col] = 'v2'\n",
    "            \n",
    "    # IDENTIFIER_AFFILIATION in v2, not in v3 - not checking as dropped from output\n",
    "    \n",
    "    # blank samples conventions\n",
    "    v3_plate_only = ((df['TUBE_OR_WELL_ID'] == 'PLATE_ONLY_1_BLANK').any()\n",
    "                     or (df['TUBE_OR_WELL_ID'] == 'PLATE_ONLY_2_BLANKS').any())\n",
    "    if (df['TUBE_OR_WELL_ID'] == 'PLATE_ONLY').any():\n",
    "        if v3_plate_only:\n",
    "            logging.error('found both bioscan v2 and v3 style PLATE_ONLY entries in manifest')\n",
    "        else:\n",
    "            version_evidence['PLATE_ONLY'] = 'v2'\n",
    "    elif v3_plate_only:\n",
    "        version_evidence['PLATE_ONLY'] = 'v3'\n",
    "    \n",
    "    # conflicting evidence\n",
    "    if len(set(version_evidence.values())) > 1:\n",
    "        msg = 'found conflicting evidence on bioscan version: '\n",
    "        for k, v in version_evidence.items():\n",
    "            msg += f'{k} suggests {v}, '\n",
    "        msg += f'using {version_evidence[\"CATCH_SOLUTION\"]} based on CATCH_SOLUTION column'\n",
    "        logging.error(msg)\n",
    "        # raise ValueError('fix bioscan manifest version before proceeding')\n",
    "        \n",
    "    v = version_evidence['CATCH_SOLUTION']\n",
    "    logging.info(f'bioscan manifest {v} inferred')\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_series(df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID'):\n",
    "    \n",
    "    # series should be 1,2, ..., nsamples\n",
    "    logging.debug('validating SERIES column')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error(f'duplicate SERIES column values: {df.index[df.index.duplicated()].to_list()}, '\n",
    "                      f'references to these SERIES can be wrong')\n",
    "    \n",
    "    # exclude non-numeric SERIES - these don't work with ranges\n",
    "    series_numeric = df.index.astype(str).str.isnumeric()\n",
    "    if not series_numeric.all():\n",
    "        logging.error(f'found rows with non-numeric SERIES column values: '\n",
    "                      f'{df.index[~series_numeric].to_list()} - '\n",
    "                      f'these will be excluded from validation and output')\n",
    "        df = df.loc[series_numeric]\n",
    "    \n",
    "    # exclude empty rows based on empty \n",
    "    empty_rows = (df[plate_col] == '') | (df[well_col] == '')\n",
    "    if empty_rows.any():\n",
    "        logging.info(f'found and excluded {empty_rows.sum()} rows because {plate_col} or {well_col} is empty')\n",
    "        df = df.loc[~empty_rows]\n",
    "        \n",
    "    # check the remaining SERIES are continuous\n",
    "    expected_series = set([str(i) for i in range(1, df.shape[0] + 1)])\n",
    "    observed_series = set(df.index.astype(str))\n",
    "    if expected_series != observed_series:\n",
    "        logging.error(f'SERIES column values {sorted(list(expected_series - observed_series))} are missing, '\n",
    "                      f'{sorted(list(observed_series - expected_series))} are unexpected')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# df = validate_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_ranges(df):\n",
    "    \n",
    "    # based on https://stackoverflow.com/questions/4628333/converting-a-list-of-integers-into-range-in-python\n",
    "    i = df.index.astype(int).to_list()\n",
    "    ranges = []\n",
    "    for a, b in itertools.groupby(enumerate(i), lambda pair: pair[1] - pair[0]):\n",
    "        b = list(b)\n",
    "        if b[0][1] != b[-1][1]:\n",
    "            ranges.append(f'{b[0][1]}-{b[-1][1]}')\n",
    "        else:\n",
    "            ranges.append(f'{b[0][1]}')\n",
    "            \n",
    "    return ', '.join(ranges)\n",
    "\n",
    "# index_ranges(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_spaces(df, title):\n",
    "    \n",
    "    trailing_spaces_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        nonbreaking_spaces = df[col].str.contains(u\"\\u00A0\")\n",
    "        if nonbreaking_spaces.any():\n",
    "            logging.info(f'non-breaking spaces found in {col} column, '\n",
    "                         f'SERIES {index_ranges(df.loc[nonbreaking_spaces])} - '\n",
    "                         'replacing with spaces for validation and output')\n",
    "            df[col] = df[col].str.replace(u\"\\u00A0\", \" \")\n",
    "        \n",
    "        newlines = df[col].str.contains(\"\\n\")\n",
    "        if newlines.any():\n",
    "            logging.info(f'line breaks found in {col} column, '\n",
    "                         f'SERIES {index_ranges(df.loc[newlines])} - '\n",
    "                         'replacing with spaces for validation and output')\n",
    "            df[col] = df[col].str.replace(\"\\n\", \" \")\n",
    "        \n",
    "        stripped_col = df[col].str.strip()\n",
    "        trailing_spaces = (df[col] != stripped_col)\n",
    "        if trailing_spaces.any():\n",
    "            logging.debug(f'trailing spaces found in {col} column, '\n",
    "                         f'SERIES {index_ranges(df.loc[trailing_spaces])} - removing for validation and output')\n",
    "            df[col] = stripped_col\n",
    "            trailing_spaces_cols.append(col)\n",
    "            \n",
    "    if len(trailing_spaces_cols) > 0:\n",
    "        logging.info(f'removed some trailing spaces from the {title} manifest in '\n",
    "                     f'columns {\", \".join(trailing_spaces_cols)}')\n",
    "            \n",
    "    return df\n",
    "\n",
    "# df = remove_trailing_spaces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_columns(df, template_df, bioscan_version='v3', optional_columns=['MISC_METADATA']):\n",
    "    \n",
    "    logging.debug('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "    optional_cols = set(optional_columns)\n",
    "    \n",
    "    # assuming validation against v3 template\n",
    "    if bioscan_version == 'v2':\n",
    "        template_cols.remove('CATCH_SOLUTION')\n",
    "        template_cols.remove('AMOUNT_OF_CATCH_PLATED')\n",
    "        template_cols.update(['IDENTIFIER_AFFILIATION'])\n",
    "    \n",
    "    extra_cols = data_cols - template_cols\n",
    "    if extra_cols != set():\n",
    "        logging.info(f'extra columns in filled manifest compared to template: {extra_cols}')\n",
    "        for col in extra_cols:\n",
    "            if col != col.upper().replace(' ','_'):\n",
    "                logging.warning(f'extra column name \"{col}\" is not upppercase or contains spaces')\n",
    "    # do not report missing optional columns\n",
    "    if template_cols - data_cols - optional_cols != set():\n",
    "        logging.error(f'template columns missing from filled manifest: {template_cols - data_cols}')\n",
    "\n",
    "# check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_dict(fn, validation_sheet='Data Validation - do not edit'):\n",
    "    \n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.debug(f'extracting value validation data from {fn} sheet \"{validation_sheet}\"')\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name=validation_sheet)\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "    \n",
    "    return valid_dict\n",
    "\n",
    "# valid_dict = get_valid_dict(anospp_fn, validation_sheet='TAB 5 Data Validation - do not ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_missing(series, na_values=[]):\n",
    "    \n",
    "    # valid missing data \n",
    "    if len(na_values) > 0:\n",
    "        no_data = (series.isin(na_values))\n",
    "        logging.debug(f'excluding {no_data.sum()} samples with missing values ({na_values}) '\n",
    "                      f'in column {series.name}')\n",
    "        return series[~no_data]\n",
    "    return series\n",
    "    \n",
    "# exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED',''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_contributors(fn, contrib_sheet='TAB 1 Contributors'):\n",
    "    \n",
    "    logging.debug(f'validating contributors in {fn}')\n",
    "        \n",
    "    df = pd.read_excel(fn, dtype=str, keep_default_na=False,\n",
    "                       sheet_name=contrib_sheet)\n",
    "    \n",
    "    df = remove_trailing_spaces(df, title='contributors')\n",
    "    \n",
    "    if 'EMAIL ADDRESS' in df.columns:\n",
    "        logging.warning('replacing \"EMAIL ADDRESS\" with \"EMAIL_ADDRESS\" contributors column name')\n",
    "        df.rename(columns={\n",
    "               \"EMAIL ADDRESS\": \"EMAIL_ADDRESS\",\n",
    "        }, inplace=True)\n",
    "\n",
    "    expected_columns = ['SURNAME','FIRST_NAME','PRIMARY_AFFILIATION','EMAIL_ADDRESS','CONTRIBUTION','CONFIRMATION']\n",
    "    \n",
    "    if set(df.columns) != set(expected_columns):\n",
    "        ec = set(df.columns) - set(expected_columns)\n",
    "        mc = set(expected_columns) - set(df.columns)\n",
    "        logging.error(f'mismatch in contributor columns: extra {ec}, missing {mc}')\n",
    "    \n",
    "    for delim_char in (';','|'):\n",
    "        for col in df.columns:\n",
    "            if df[col].str.contains(delim_char, regex=False).any():\n",
    "                logging.error(f'contributor column {col} contains delimiter \"{delim_char}\"')\n",
    "    \n",
    "    df['FULL_NAME'] = df['FIRST_NAME'] + ' ' + df['SURNAME']\n",
    "    df['PARTNER_CODE'] = (df['SURNAME'].str.slice(0,2) + df['FIRST_NAME'].str.slice(0,2)).str.upper()\n",
    "    is_template_name = (df['FULL_NAME'] == 'Charles R. Darwin')\n",
    "    if is_template_name.any():\n",
    "        logging.warning('suspect template contributor was not removed')\n",
    "    \n",
    "    is_dup_name = df['FULL_NAME'].duplicated()\n",
    "    if is_dup_name.any():\n",
    "        logging.error('duplicated contributor names {}'.format(\n",
    "                df.loc[is_dup_name, 'FULL_NAME'].to_list()))\n",
    "    \n",
    "    is_valid_email = df['EMAIL_ADDRESS'].str.match(r'^[A-z0-9._%+-]+@[A-z0-9.-]+\\.[A-z]{2,}$')\n",
    "    if not is_valid_email.all():\n",
    "        logging.error('invalid contributor email addresses {}'.format(\n",
    "                df.loc[~is_valid_email, 'EMAIL_ADDRESS'].to_list()))\n",
    "    \n",
    "    is_confirmed = (df['CONFIRMATION'] == 'YES')\n",
    "    if not is_confirmed.any():\n",
    "        logging.error('confirmation lacking for any of contributors')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "# contrib_df = validate_contributors(anospp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_regex(col, df, na_values=[], extra_msg=''):\n",
    "    \n",
    "    logging.debug(f'validating data format in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # dates between 1900 and 2100\n",
    "    date_regex = (r'^(19\\d\\d|20\\d\\d)-(0[1-9]|1[0-2])-(0[1-9]|[12]\\d|3[01])$|'\n",
    "                  r'^(19\\d\\d|20\\d\\d)-(0[1-9]|1[0-2])$|'\n",
    "                  r'^(19\\d\\d|20\\d\\d)$')\n",
    "    \n",
    "    numeric_regex = r'^[-+]?\\d*\\.?\\d+$'\n",
    "    \n",
    "    regexs = {\n",
    "        'CATCH_LOT': (r'^C\\d{3}[A-Z]$|^NOT_APPLICABLE$', \n",
    "                      'like C123A or NOT_APPLICABLE'),\n",
    "        'DATE_OF_COLLECTION': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'DECIMAL_LATITUDE': (r'^[-+]?([0-8]\\d|\\d)(\\.\\d{1,6})?$', \n",
    "                             'between -90 and 90 with up to 6 decimals'),\n",
    "        'DECIMAL_LONGITUDE': (r'^[-+]?(1[0-7]\\d|\\d\\d|\\d)(\\.\\d{1,6})?$', \n",
    "                              'between -180 and 180 with up to 6 decimals'),\n",
    "        'WHAT_3_WORDS': (r'^///[a-z]+\\.[a-z]+\\.[a-z]+$', \n",
    "                         'like ///one.two.three'),\n",
    "        'TIME_OF_COLLECTION': (r'^(?:[01]\\d|2[0-3]):[0-5]\\d$|^(?:[01]\\d|2[0-3]):[0-5]\\d:[0-5]\\d$', \n",
    "                              'in HH:MM format'),\n",
    "        'DURATION_OF_COLLECTION': (r'^P(?:\\d+Y)?(?:\\d+M)?(?:\\d+D)?(?:T(?:\\d+H)?(?:\\d+M)?(?:\\d+S)?)?$', \n",
    "                                  'in P[n]Y[n]M[n]DT[n]H[n]M[n]S format'),\n",
    "        'DATE_OF_PLATING': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'DATE_OF_PRESERVATION': (date_regex, 'in YYYY-MM-DD format'),\n",
    "        'ELEVATION': (numeric_regex, 'only a number (in metres)'),\n",
    "        'DNA_EXTRACT_VOLUME_PROVIDED': (numeric_regex, 'only a number (in microlitres)'),\n",
    "        'DNA_EXTRACT_CONCENTRATION': (numeric_regex, 'only a number (in nanograms per microlitre)')\n",
    "    }\n",
    "    \n",
    "    is_valid_regex = series.str.match(regexs[col][0])\n",
    "    if not is_valid_regex.all():\n",
    "        offending_values = list(series[~is_valid_regex].unique())\n",
    "        s = index_ranges(series[~is_valid_regex])\n",
    "        msg = (f'{col} format{extra_msg} incorrect for SERIES {s}: found {offending_values} - '\n",
    "              f'expected to be {regexs[col][1]}')\n",
    "        if col == 'CATCH_LOT':\n",
    "            logging.warning(msg)\n",
    "        else:\n",
    "            logging.error(msg)\n",
    "    # extra check for duration of collection\n",
    "    if col == 'DURATION_OF_COLLECTION':\n",
    "        is_recommended_regex = series.str.match(r'^PT(\\d+)H$')\n",
    "        if not is_recommended_regex.all():\n",
    "            offending_values = list(series[~is_recommended_regex & is_valid_regex].unique())\n",
    "            s = index_ranges(series[~is_recommended_regex & is_valid_regex])\n",
    "            msg = (f'{col} format suggestion{extra_msg} for SERIES {s}: found {offending_values} - '\n",
    "                  f'we ask to use PT[n]H format (hours only)')\n",
    "            logging.warning(msg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_plates_wells(df, contrib_df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID', \n",
    "                          bioscan=False, bioscan_version='v3'):\n",
    "    \n",
    "    bioscan_partners_fn = '../data/bioscan_partners.tsv'\n",
    "    \n",
    "    # expect only complete 96-well plates\n",
    "    logging.debug(f'validating {plate_col} and {well_col} columns')\n",
    "        \n",
    "    # plate names validation\n",
    "    plates = df[plate_col].drop_duplicates()\n",
    "    plate_name_template = '^[A-Z]{4}_[0-9]{3}$'\n",
    "    wrong_plate_names = plates[~plates.str.match(plate_name_template, na='')]\n",
    "    if len(wrong_plate_names) > 0:\n",
    "        logging.error(f'plate names {wrong_plate_names.to_list()} do not match template ABCD_1234')\n",
    "    # check plate name prefixes against partner codes\n",
    "    # and assign GAL\n",
    "    if bioscan:\n",
    "        # strip TOL prefix from plates\n",
    "        plates = plates.str.replace('TOL-', '', regex=True)\n",
    "    plate_prefixes = plates.str.slice(0,4)\n",
    "    if bioscan:\n",
    "        partners_df = pd.read_csv(bioscan_partners_fn, sep='\\t', dtype=str)\n",
    "        assert partners_df['partner_code'].is_unique, f'duplicated partner codes found in {bioscan_partners_fn}'\n",
    "        unknown_prefixes = (~plate_prefixes.isin(partners_df['partner_code']))\n",
    "        \n",
    "        possible_partner_codes = plate_prefixes.value_counts().index.to_list()\n",
    "        if len(possible_partner_codes) > 1:\n",
    "            logging.error(f'only one plate ID prefix expected, found multiple: {possible_partner_codes}')\n",
    "        # selecting most frequent prefix as partner code\n",
    "        selected_partner_code = possible_partner_codes[0]\n",
    "        selected_partner_df = partners_df.query(f'partner_code == \"{selected_partner_code}\"')\n",
    "        if selected_partner_df.shape[0] == 0:\n",
    "            logging.error(f'plate name prefix {selected_partner_code} not found in {bioscan_partners_fn} '\n",
    "                          f'as partner code, using \"Sanger Institute\" as default partner')\n",
    "            gal = \"Sanger Institute\"\n",
    "        else:\n",
    "            gal = selected_partner_df['gal'].iloc[0]\n",
    "        logging.info(f'selected GAL \"{gal}\" for partner code \"{selected_partner_code}\"')\n",
    "    else:\n",
    "        # anospp\n",
    "        unknown_prefixes = (~plate_prefixes.isin(contrib_df['PARTNER_CODE']))\n",
    "        selected_partner_code = 'SANG'\n",
    "        gal = \"Sanger Institute\"\n",
    "    if unknown_prefixes.any():\n",
    "        logging.error(f'plate ID prefixes not recognised for {plates[unknown_prefixes].to_list()}')\n",
    "        \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (r,c) in itertools.product(row_id, col_id)]\n",
    "    \n",
    "    # different plate-only values between bioscan versions\n",
    "    if bioscan_version == 'v2':\n",
    "        plate_only_vals = ['PLATE_ONLY']\n",
    "    elif bioscan_version == 'v3':\n",
    "        plate_only_vals = ['PLATE_ONLY_1_BLANK','PLATE_ONLY_2_BLANKS']\n",
    "    else:\n",
    "        raise ValueError(f'unhandled bioscan version {bioscan_version} at validate_plates_wells()') \n",
    "        \n",
    "    for plate, pdf in df.groupby(plate_col):\n",
    "        \n",
    "        if bioscan and (pdf[well_col].isin(plate_only_vals)).any():\n",
    "            logging.debug(f'skipping well validation for PLATE_ONLY plate {plate}')\n",
    "            continue\n",
    "        \n",
    "        # check for well duplicates\n",
    "        dup_wells = pdf[well_col].duplicated()\n",
    "        if dup_wells.any():\n",
    "            logging.error(f'duplicate {well_col} for plate {plate}: {pdf.loc[dup_wells, well_col].unique()}')\n",
    "        # check for non A1...H12 wells\n",
    "        observed_wells_set = set(pdf[well_col])\n",
    "        expected_wells_set = set(expected_wells)\n",
    "        if observed_wells_set != expected_wells_set:\n",
    "            msg = f'in {well_col} for plate {plate}, '\n",
    "            if len(expected_wells_set - observed_wells_set) > 0:\n",
    "                msg += f'wells {expected_wells_set - observed_wells_set} are missing'\n",
    "            if len(observed_wells_set - expected_wells_set) > 0:\n",
    "                if msg.endswith('missing'):\n",
    "                    msg += ', '\n",
    "                msg += f'wells {observed_wells_set - expected_wells_set} are excessive'\n",
    "            logging.error(msg)\n",
    "            \n",
    "    n_plate_only = df[well_col].isin(plate_only_vals).sum()\n",
    "    \n",
    "    logging.info(f'found {df.shape[0] - n_plate_only} samples across '\n",
    "                 f'{df[plate_col].nunique() - n_plate_only} plates and '\n",
    "                 f'{n_plate_only} plate-only entries')\n",
    "    \n",
    "\n",
    "    return df, gal, selected_partner_code\n",
    "        \n",
    "# df = validate_plates_wells(df, contrib_df, 'RACK_OR_PLATE_ID', 'TUBE_OR_WELL_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_plate_only(df, plate_col='RACK_OR_PLATE_ID', well_col='TUBE_OR_WELL_ID'):\n",
    "    \n",
    "    # add 96-well plate well IDs for expansion\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    expected_wells = [r + str(c) for (c,r) in itertools.product(col_id, row_id)]\n",
    "    \n",
    "    blank_wells = {\n",
    "        'PLATE_ONLY':['G12','H12'], # v2\n",
    "        'PLATE_ONLY_1_BLANK':['H12'], # v3\n",
    "        'PLATE_ONLY_2_BLANKS':['G12','H12'], # v3\n",
    "    }\n",
    "    \n",
    "    if df[well_col].isin(blank_wells.keys()).any():\n",
    "        pdfs = []\n",
    "\n",
    "        for plate, pdf in df.groupby(plate_col):\n",
    "            # If plate level only metadata is being entered\n",
    "            # put “PLATE_ONLY” in well \n",
    "            # and use only one row to capture the metadata for the whole plate\n",
    "            if pdf[well_col].isin(blank_wells.keys()).any():\n",
    "                \n",
    "                if pdf.shape[0] > 1:\n",
    "                    logging.error(f'expected one row in PLATE_ONLY plate {plate}, found {pdf.shape[0]} - '\n",
    "                                  f'extracting metadata from first row only')\n",
    "                plate_only_val = pdf[well_col].iloc[0]\n",
    "                logging.info(f'found {plate_only_val} plate {plate}, expanding to 96 well rows, '\n",
    "                             f'keeping {\" and \".join(blank_wells[plate_only_val])} blank')\n",
    "                # expand to 96 rows\n",
    "                pdf = pd.DataFrame(pdf.iloc[0] for i in range(len(expected_wells)))\n",
    "                pdf[well_col] = expected_wells\n",
    "                \n",
    "                for blank_well in blank_wells[plate_only_val]:\n",
    "                    for col in pdf.columns:\n",
    "                        if col == 'ORGANISM_PART':\n",
    "                            pdf.loc[pdf[well_col] == blank_well, col] = 'NOT_APPLICABLE'\n",
    "                        elif col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                                         'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION']:\n",
    "                            pdf.loc[pdf[well_col] == blank_well, col] = ''\n",
    "            pdfs.append(pdf)\n",
    "            \n",
    "        df = pd.concat(pdfs).reset_index(drop=True)\n",
    "        df.index += 1\n",
    "        df.index.name = 'SERIES'\n",
    "        \n",
    "    else:\n",
    "        logging.debug('no PLATE_ONLY rows found')\n",
    "    \n",
    "    return df\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_blanks(df, bioscan=False):\n",
    "    \n",
    "    logging.debug('checking and excluding blank samples based on ORGANISM_PART being NOT_APPLICABLE')    \n",
    "    \n",
    "    # blank criterion\n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    blank_df = df[is_blank]\n",
    "    \n",
    "    non_blank_df = df[~is_blank]\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    non_blank_last_well_df = non_blank_df.query('TUBE_OR_WELL_ID == \"H12\"')\n",
    "    if non_blank_last_well_df.shape[0] > 0:\n",
    "        logging.error(f'last well H12 is not blank at SERIES {index_ranges(non_blank_last_well_df)}: '\n",
    "                      f'in ORGANISM_PART, expected \"NOT_APPLICABLE\", '\n",
    "                      f'found {non_blank_last_well_df.ORGANISM_PART.to_list()}, '\n",
    "                      f'these samples will be included in further analysis',\n",
    "                        \n",
    "        )\n",
    "    \n",
    "    if bioscan:\n",
    "        non_blank_penultimate_well_df = non_blank_df.query('TUBE_OR_WELL_ID == \"G12\"')\n",
    "        if non_blank_penultimate_well_df.shape[0] > 0:\n",
    "            logging.warning(\n",
    "                f'well G12 is not blank at SERIES {index_ranges(non_blank_penultimate_well_df)}: '\n",
    "                f'in ORGANISM_PART, expected \"NOT_APPLICABLE\", '\n",
    "                f'found {set(non_blank_penultimate_well_df.ORGANISM_PART.to_list())}, '\n",
    "                f'these samples will not be sequenced',\n",
    "            )\n",
    "            \n",
    "        blank_non_term_df = blank_df[~blank_df.TUBE_OR_WELL_ID.isin(['G12','H12'])]\n",
    "        if blank_non_term_df.shape[0] > 0:\n",
    "            logging.warning(\n",
    "                f'blanks in non-G12/H12 wells at SERIES {index_ranges(blank_non_term_df)}',\n",
    "            )\n",
    "\n",
    "    any_excessive_blank_info = False\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col not in ['SERIES','CATCH_LOT','RACK_OR_PLATE_ID',\n",
    "                       'TUBE_OR_WELL_ID','ORGANISM_PART','PRESERVATIVE_SOLUTION',\n",
    "                       'OTHER_INFORMATION','MISC_METADATA']:\n",
    "            excessive_blank_info_df = blank_df[blank_df[col] != '']\n",
    "            if excessive_blank_info_df.shape[0] > 0:\n",
    "                logging.debug(f'{col} column values non-empty for blank samples '\n",
    "                              f'at SERIES {index_ranges(excessive_blank_info_df)} - these will be deleted')\n",
    "                df.loc[is_blank, col] = ''\n",
    "                any_excessive_blank_info = True\n",
    "    \n",
    "    if any_excessive_blank_info:\n",
    "        logging.info('removed some excessive information from blank samples')\n",
    "    \n",
    "    logging.info(f'found and checked {is_blank.sum()} blank samples, '\n",
    "                 f'{non_blank_df.shape[0]} non-blank samples and plate-only entries '\n",
    "                 f'will be further validated')\n",
    "    \n",
    "    \n",
    "    return df, is_blank\n",
    "\n",
    "# print(df.shape)\n",
    "# is_blank = check_blanks(df)\n",
    "# print(df[~is_blank].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=[], level='e', extra_msg=''):\n",
    "    \n",
    "    logging.debug(f'validating for allowed values in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error(f'{col} column not found in validation sheet')\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        series = series.apply(lambda v: [x.strip() for x in v.split(sep)])\n",
    "        sep_col_values = list()\n",
    "        for v in series:\n",
    "            sep_col_values.extend(v)\n",
    "        col_values = set(sep_col_values)\n",
    "    \n",
    "    valid_values = set(valid_dict[col])\n",
    "    \n",
    "    invalid_values = col_values - valid_values\n",
    "    \n",
    "    if len(invalid_values) > 0:\n",
    "        for invalid_value in invalid_values:\n",
    "            if invalid_value == '':\n",
    "                invalid_value_series = index_ranges(series[series == ''])\n",
    "            elif sep:\n",
    "                invalid_mask = series.apply(lambda v: invalid_value in v)\n",
    "                invalid_value_series = index_ranges(series[invalid_mask])\n",
    "            else:\n",
    "                invalid_value_series = index_ranges(series[series == invalid_value])\n",
    "            msg = f'invalid value{extra_msg} in {col} column, SERIES {invalid_value_series}: \"{invalid_value}\"'\n",
    "            if level == 'i':\n",
    "                logging.info(msg)\n",
    "            elif level == 'w':\n",
    "                logging.warning(msg)\n",
    "            elif level == 'e':\n",
    "                logging.error(msg)\n",
    "            \n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "# validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \", na_values=[], level='e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_asterisks(col, df):\n",
    "\n",
    "    series = df[col]\n",
    "    asterisk_to_strip = (series.str.startswith(\"*\") & series.str.endswith(\"*\"))\n",
    "    if asterisk_to_strip.any():\n",
    "        asterisk_series = index_ranges(series[asterisk_to_strip])\n",
    "        logging.info(f'{col} column, SERIES {asterisk_series}: '\n",
    "                        f'stripping asterisks from {series[asterisk_to_strip].unique()} for output')\n",
    "        series = series.str.strip('*')\n",
    "    df[col] = series\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_catch_lot_dates(df):\n",
    "    \n",
    "    logging.debug(f'checking that CATCH_LOT does not span multiple DATE_OF_COLLECTION')\n",
    "    \n",
    "    for col in ('CATCH_LOT','DATE_OF_COLLECTION'):\n",
    "        if col not in df.columns:\n",
    "            logging.error(f'{col} not found in manifest, skipping catch lot validation')\n",
    "            return\n",
    "    \n",
    "    biosc_catch_lot_df = df[df.CATCH_LOT.str.match(r'^C\\d{3}[A-Z]$')]\n",
    "    \n",
    "    for catch_lot, catch_lot_df in biosc_catch_lot_df.groupby('CATCH_LOT'):\n",
    "        if catch_lot_df.DATE_OF_COLLECTION.nunique() > 1:\n",
    "            logging.error(f'catch lot {catch_lot} entries span multiple dates - '\n",
    "                          f'{catch_lot_df.DATE_OF_COLLECTION.unique()} - one catch lot must be from one date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dates_text(before_col, after_col, df):\n",
    "    \n",
    "    logging.debug(f'checking that {before_col} is earlier than {after_col}')\n",
    "    \n",
    "    for col in (before_col, after_col):\n",
    "        if col not in df.columns:\n",
    "            logging.error(f'{col} column not found in manifest')\n",
    "            return\n",
    "    # invalid date formats or empty string converted to NaT\n",
    "    before_series = pd.to_datetime(df[before_col], format='%Y-%m-%d', errors='coerce').copy()\n",
    "    after_series = pd.to_datetime(df[after_col], format='%Y-%m-%d', errors='coerce').copy()\n",
    "    \n",
    "    date_conflict = before_series > after_series\n",
    "    \n",
    "    s = index_ranges(df[date_conflict])\n",
    "    \n",
    "    if date_conflict.any():\n",
    "        logging.error(f'{before_col} column values are later than {after_col} column values in SERIES {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_country_and_coordinates(df, fn, na_values=[''], bioscan=False):\n",
    "    \n",
    "    logging.debug('validating COUNTRY_OF_COLLECTION against DECIMAL_LATITUDE and DECIMAL_LONGITUDE')\n",
    "    \n",
    "    country_col, lat_col, lon_col = 'COUNTRY_OF_COLLECTION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[country_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error(f'some of {country_col} {lat_col} {lon_col} columns not found in manifest')\n",
    "        return\n",
    "    loc_df_isna = (loc_df_complete.isin(na_values)).all(axis=1)\n",
    "    if loc_df_isna.any():\n",
    "        logging.info(f'removing {loc_df_isna.sum()} missing data ({na_values}) samples from coordinate analysis')\n",
    "    loc_df_complete = loc_df_complete[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn + '_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.debug('querying coordinates')\n",
    "        locator = Nominatim(user_agent='bioscanManifestTest')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            coord_list = c.split(', ')\n",
    "            lat = coord_list[0]\n",
    "            lon = coord_list[1]\n",
    "            try:\n",
    "                lat, lon = float(lat), float(lon)\n",
    "                # na\n",
    "                assert lat == lat\n",
    "                assert lon == lon\n",
    "            except:\n",
    "                unparsed_df = df[(df[lat_col] == str(lat)) & df[lon_col] == str(lon)]\n",
    "                logging.error(\n",
    "                    f'problem parsing coordinates {c} at SERIES {index_ranges(unparsed_df)}'\n",
    "                )\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error(\n",
    "                    f'invalid DECIMAL_LATITUDE {lat} at SERIES {index_ranges(df[df[lat_col] == str(lat)])}'\n",
    "                    f', should be in [-90,90]')\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error(\n",
    "                    f'invalid DECIMAL_LONGITUDE {lon} at SERIES {index_ranges(df[df[lon_col] == str(lon)])}'\n",
    "                    f', should be in [-180,180]')\n",
    "                continue\n",
    "                \n",
    "            # web query\n",
    "            logging.debug(f'querying with \"{c}\"')\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[country_col].str.strip().str.upper()\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        try:\n",
    "            lat, lon = coord.split(', ')\n",
    "        except:\n",
    "                logging.error(\n",
    "                    f'problem parsing coordinates {coord} in locations results'\n",
    "                )\n",
    "                continue\n",
    "        coord_series = index_ranges(df.query(f'({lat_col} == \"{lat}\") & ({lon_col} == \"{lon}\")'))\n",
    "                    \n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error(\n",
    "                f'multiple COUNTRY_OF_COLLECTION records found for coordinates {coord}, SERIES {coord_series}: '\n",
    "                f'{partner_countries.unique()}, skipping coordinate validation'\n",
    "            )\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error(f'no COUNTRY_OF_COLLECTION records found for coordinates {coord}, SERIES {coord_series}')\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning(f'could not locate country for coordinates {coord}, '\n",
    "                            f'COUNTRY_OF_COLLECTION {partner_country}, SERIES {coord_series}')\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error(f'country mismatch for coordinates {coord}, SERIES {coord_series}: '\n",
    "                          f'COUNTRY_OF_COLLECTION indicated as {partner_country}, '\n",
    "                          f'while coordinates point to {coord_country}')\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "# loc_test = validate_country_and_coordinates(df, anospp_fn)\n",
    "# loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_taxonomy(df, ncbi, na_values = [], anospp=False, add_taxids=False):\n",
    "\n",
    "    logging.debug('validating taxonomy against NCBI')\n",
    "    \n",
    "    if anospp:\n",
    "        df['PREDICTED_ORDER_OR_GROUP'] = 'Diptera'\n",
    "        df['PREDICTED_FAMILY'] = 'Culicidae'\n",
    "        df['PREDICTED_GENUS'] = 'Anopheles'\n",
    "        \n",
    "        harbach_spp = []\n",
    "        with open('../data/harbach_spp_201910.txt') as f:\n",
    "            for line in f:\n",
    "                harbach_spp.append('Anopheles ' + line.strip())\n",
    "        harbach_spp = set(harbach_spp)\n",
    "        \n",
    "    tax_levels = {\n",
    "        'PREDICTED_ORDER_OR_GROUP':'order',\n",
    "        'PREDICTED_FAMILY':'family',\n",
    "        'PREDICTED_GENUS':'genus',\n",
    "        'PREDICTED_SCIENTIFIC_NAME':'species'\n",
    "    }\n",
    "    \n",
    "    expected_ranks = {\n",
    "            'order':('subphylum','class','subclass','superorder','order','suborder'),\n",
    "            'family':('suborder','superfamily','family','subfamily','tribe','subtribe'),\n",
    "            'genus':('genus','subgenus'),\n",
    "            'species':('species')\n",
    "        }\n",
    "    \n",
    "    hierarchies = df[tax_levels.keys()].drop_duplicates().copy()\n",
    "    \n",
    "    hierarchies.columns = list(tax_levels.values())\n",
    "        \n",
    "    tax_info = dict()\n",
    "    \n",
    "    for tax_col, tax_level in tax_levels.items():\n",
    "        \n",
    "        logging.debug(f'validating {tax_col} column against NCBI')\n",
    "        \n",
    "        if tax_col not in df.columns:\n",
    "                logging.error(f'{tax_col} column not found in manifest')\n",
    "                continue\n",
    "            \n",
    "        tax_names = list(hierarchies[tax_level].unique())\n",
    "        \n",
    "        for na_value in na_values:\n",
    "            try:\n",
    "                tax_names.remove(na_value)\n",
    "            except:\n",
    "                pass \n",
    "            \n",
    "        for i, tax_name in enumerate(tax_names):\n",
    "            if len(tax_name) == 0:\n",
    "                continue\n",
    "            corr_tax_name = tax_name[0].upper() + tax_name[1:].lower()\n",
    "            if corr_tax_name != tax_name and tax_name != 'blank sample':\n",
    "                s = index_ranges(df.query(f'{tax_col} == \"{tax_name}\"'))\n",
    "                logging.info(f'{tax_col} column, SERIES {s}'\n",
    "                             f': unexpected case for \"{tax_name}\", '\n",
    "                             f'changing to \"{corr_tax_name}\" for validation and output')\n",
    "            tax_names[i] = corr_tax_name\n",
    "        \n",
    "        tax_info[tax_level] = ncbi.get_name_translator(tax_names) \n",
    "        \n",
    "        unmatched_names = set(tax_names) - set(tax_info[tax_level].keys())\n",
    "        if len(unmatched_names) > 0:\n",
    "            for tname in unmatched_names:\n",
    "                s = index_ranges(df[df[tax_col].str.match(f'^{re.escape(tname)}$', case=False)])\n",
    "                if tax_level == 'species' and anospp:\n",
    "                    if tname in harbach_spp:\n",
    "                        logging.warning(f'{tax_col} column, SERIES {s}:'\n",
    "                                        f' \"{tname}\" found in Harbach list, but not in NCBI Taxonomy')\n",
    "                    else:\n",
    "                        logging.error(f'{tax_col} column, SERIES {s}'\n",
    "                                      f': \"{tname}\" not found in both Harbach list and NCBI Taxonomy')\n",
    "                else:\n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: \"{tname}\" not found in NCBI Taxonomy')\n",
    "        \n",
    "        \n",
    "        \n",
    "        for tname, tids in tax_info[tax_level].items():\n",
    "            \n",
    "            ranks = ncbi.get_rank(tids)\n",
    "            \n",
    "            upd_tid = tids[0]\n",
    "            \n",
    "            s = index_ranges(df[df[tax_col].str.match(f'^{re.escape(tname)}$', case=False)])\n",
    "                        \n",
    "            if len(tids) == 1:\n",
    "                if ranks[upd_tid] not in expected_ranks[tax_level]: \n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: found unexpected rank for \"{tname}\" '\n",
    "                                      f'(taxid {upd_tid}): \"{ranks[upd_tid]}\"')\n",
    "                    \n",
    "            if len(tids) > 1:            \n",
    "                for tid, rank in ranks.items():\n",
    "                    if rank in expected_ranks[tax_level] and len(tids) > 1:\n",
    "                        logging.debug(f'{tax_col} column, SERIES {s}: using only first matching rank '\n",
    "                                      f'for \"{tname}\" (taxid {tid}): \"{rank}\"')\n",
    "                        upd_tid = tid\n",
    "                        break\n",
    "                else:\n",
    "                    logging.error(f'{tax_col} column, SERIES {s}: could not find matching rank '\n",
    "                                  f'for \"{tname}\" - using taxid {upd_tid}: \"{ranks[upd_tid]}\"')\n",
    "                    \n",
    "            tax_info[tax_level][tname] = upd_tid\n",
    "        \n",
    "        #logging.info(f'{tax_level} {tax_info[tax_level]}')\n",
    "                    \n",
    "    # check consistency of taxonomy\n",
    "    for _, r in hierarchies.iterrows():\n",
    "        \n",
    "        if r.order in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            order_id = tax_info['order'][r.order]\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_ORDER_OR_GROUP value \"{r.order}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            family_id = tax_info['family'][r.family]\n",
    "            \n",
    "            family_lineage = ncbi.get_lineage(family_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_FAMILY'].str.match(f'^{re.escape(r.family)}$', case=False)])\n",
    "            \n",
    "            if order_id not in family_lineage:\n",
    "                logging.error(f'PREDICTED_FAMILY column, SERIES {s}: family \"{r.family}\" (taxid {family_id}) '\n",
    "                              f'does not belong to order \"{r.order}\" (taxid {order_id})')\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_FAMILY value \"{r.family}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.genus in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            genus_id = tax_info['genus'][r.genus]\n",
    "            \n",
    "            genus_lineage = ncbi.get_lineage(genus_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_GENUS'].str.match(f'^{re.escape(r.genus)}$', case=False)])\n",
    "            \n",
    "            if order_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_GENUS column, SERIES {s}: '\n",
    "                    f'genus \"{r.genus}\" (taxid {genus_id}) does not belong to \"{r.order}\" (taxid {order_id})')\n",
    "            if family_id not in genus_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_GENUS column, SERIES {s}: '\n",
    "                    f'genus \"{r.genus}\" (taxid {genus_id}) does not belong to \"{r.family}\" (taxid {family_id})')\n",
    "        except KeyError:\n",
    "            logging.debug(f'PREDICTED_GENUS value \"{r.genus}\" not in NCBI Taxonomy, '\n",
    "                          f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "            \n",
    "        if r.species in na_values:\n",
    "            continue\n",
    "        try:\n",
    "            species_id = tax_info['species'][r.species]\n",
    "            \n",
    "            species_lineage = ncbi.get_lineage(species_id)\n",
    "            \n",
    "            s = index_ranges(df[df['PREDICTED_SCIENTIFIC_NAME'].str.match(f'^{re.escape(r.species)}$', case=False)])\n",
    "            \n",
    "            if order_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.order}\" (taxid {order_id})')\n",
    "            if family_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.family}\" (taxid {family_id})')\n",
    "            if genus_id not in species_lineage:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) does not belong to \"{r.genus}\" (taxid {genus_id})')\n",
    "            elif r.species.split(' ')[0] != r.genus:\n",
    "                logging.error(\n",
    "                    f'PREDICTED_SCIENTIFIC_NAME column, SERIES {s}: '\n",
    "                    f'species \"{r.species}\" (taxid {species_id}) states a different genus name than '\n",
    "                    f'\"{r.genus}\" (taxid {genus_id})')\n",
    "        except KeyError:\n",
    "            logging.info(f'PREDICTED_SCIENTIFIC_NAME value \"{r.species}\" not in NCBI Taxonomy, '\n",
    "                         f'skipping taxonomy consistency check')\n",
    "            continue\n",
    "    \n",
    "    if add_taxids:\n",
    "        for tc in tax_levels.keys():\n",
    "            df[f'{tc}_TAXID'] = df[tc].replace(tax_info[tax_levels[tc]])\n",
    "    \n",
    "    # only fix taxonomy case after validation to get naming \n",
    "    for tax_col in tax_levels.keys():\n",
    "        df[tax_col] = df[tax_col].str.capitalize()\n",
    "            \n",
    "    return df\n",
    "        \n",
    "                \n",
    "# validate_taxonomy(df, ncbi, anospp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_specimen_id_risk(df):\n",
    "    \n",
    "    logging.debug(f'validating SPECIMEN_IDENTITY_RISK column')\n",
    "    \n",
    "    if 'SPECIMEN_IDENTITY_RISK' not in df.columns:\n",
    "        logging.error(f'SPECIMEN_IDENTITY_RISK column not found in manifest')\n",
    "        return\n",
    "    \n",
    "    # missing species name, but no idenitity risk\n",
    "    invalid_risk = ((df.PREDICTED_SCIENTIFIC_NAME == '') & (df.SPECIMEN_IDENTITY_RISK == 'N'))\n",
    "    \n",
    "    if invalid_risk.any():\n",
    "        logging.error(f'SERIES {index_ranges(df.loc[invalid_risk])}: with no PREDICTED_SCIENTIFIC_NAME, '\n",
    "                      f'SPECIMEN_IDENTITY_RISK values should be blank, not \"N\"')\n",
    "\n",
    "# validate_specimen_id_risk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_identifier(col, df, contrib_df, na_values=['']):\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    expected_people = contrib_df['FULL_NAME'].to_list()\n",
    "    \n",
    "    for person in series.unique():\n",
    "        if not (person in expected_people):\n",
    "            s = index_ranges(series[series == person])\n",
    "            \n",
    "            logging.warning(f'{col} column, SERIES {s}: \"{person}\" not found in '\n",
    "                            f'contributors sheet among {expected_people}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_freetext(col, df, na_values=['']):\n",
    "    \n",
    "    logging.debug(f'validating freetext field characters in {col} column')\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error(f'{col} column not found in manifest')\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    regex = '^[A-z0-9.,\\-_ ]+$'\n",
    "    \n",
    "    is_valid_freetext = series.str.match(regex)\n",
    "    if not is_valid_freetext.all():\n",
    "        s = index_ranges(series.loc[~is_valid_freetext])\n",
    "        logging.debug(f'{col} column, SERIES {s}: found non-standard characters - regex: \"{regex}\"')\n",
    "\n",
    "        \n",
    "# validate_freetext('IDENTIFIED_HOW', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_input_filename(input_fn, partner_code, bioscan_version):\n",
    "    \n",
    "    # ABCD_YYMM_\n",
    "    v = bioscan_version.strip('v')\n",
    "    fn_regex = fr'^{partner_code}_(2[0-4])(0[1-9]|1[0-2])_BIOSCAN_Manifest_V{v}.*.xlsx$'\n",
    "    \n",
    "    fn_basename = os.path.basename(input_fn)\n",
    "    \n",
    "    if not re.match(fn_regex, fn_basename):\n",
    "        logging.warning(f'input filename {fn_basename} does not match '\n",
    "                        f'{partner_code}_YYMM_BIOSCAN_Manifest_V{v}*.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sts_cols(df, contrib_df, gal, bioscan=True, v='v2'):\n",
    "    \n",
    "    logging.debug('adding STS columns to manifest')\n",
    "    \n",
    "    is_blank = (df['ORGANISM_PART'] == 'NOT_APPLICABLE')\n",
    "    \n",
    "    df['SPECIMEN_ID'] = df['RACK_OR_PLATE_ID'] + '_' + df['TUBE_OR_WELL_ID']\n",
    "    dup_specimen_id = df['SPECIMEN_ID'].duplicated()\n",
    "    if dup_specimen_id.any():\n",
    "        logging.error(f'duplicate SPECIMEN_ID generated: {df.SPECIMEN_ID[dup_specimen_id].unique()}')\n",
    "    df['SCIENTIFIC_NAME'] = 'unidentified'\n",
    "    df.loc[is_blank, 'SCIENTIFIC_NAME'] = 'blank sample'\n",
    "    df['TAXON_ID'] = '32644'\n",
    "    df.loc[is_blank, 'TAXON_ID'] = '2582415'\n",
    "    df['GAL'] = gal\n",
    "    df['SYMBIONT'] = 'TARGET'\n",
    "    df['REGULATORY_COMPLIANCE'] = 'Y'\n",
    "    df['HAZARD_GROUP'] = 'HG1'\n",
    "    if bioscan and v == 'v2':\n",
    "        logging.info('auto-filling CATCH_SOLUTION as 100%_ETHANOL and AMOUNT_OF_CATCH_PLATED as '\n",
    "                     'ALL_SPECIMENS_PLATED columns for bioscan manifest v2')\n",
    "        df['CATCH_SOLUTION'] = '100%_ETHANOL'\n",
    "        df['AMOUNT_OF_CATCH_PLATED'] = 'ALL_SPECIMENS_PLATED'\n",
    "        if (df['IDENTIFIER_AFFILIATION'] != '').any():\n",
    "            logging.warning('IDENTIFIER_AFFILIATION has some data filled in - '\n",
    "                            'note this column will be removed from output')\n",
    "        logging.info('dropping IDENTIFIER_AFFILIATION column for bioscan manifest v2')\n",
    "        df = df.drop(columns=['IDENTIFIER_AFFILIATION'])\n",
    "    # add contributors - delimiters checked in validate_contributors\n",
    "    if bioscan:\n",
    "        contrib_series = contrib_df['FULL_NAME'] + ';' + \\\n",
    "            contrib_df['PRIMARY_AFFILIATION'] + ';' + \\\n",
    "            contrib_df['EMAIL_ADDRESS'] + ';' + \\\n",
    "            contrib_df['CONTRIBUTION']\n",
    "        df['CONTRIBUTORS'] = '|'.join(list(contrib_series))\n",
    "    # add supplier sample name prefixes to control samples\n",
    "#     df['BIOSCAN_SUPPLIER_SAMPLE_NAME'] = df['SPECIMEN_ID']\n",
    "#     df.loc[is_blank, 'BIOSCAN_SUPPLIER_SAMPLE_NAME'] = 'CONTROL_NEG_LYSATE_' + df['SPECIMEN_ID']\n",
    "#     df.loc[is_blank & (df['TUBE_OR_WELL_ID'] == 'G12'),\n",
    "#            'BIOSCAN_SUPPLIER_SAMPLE_NAME'] = 'CONTROL_POS_' + df['SPECIMEN_ID']\n",
    "    \n",
    "    if 'MISC_METADATA' in df.columns:\n",
    "        logging.info('dropping MISC_METADATA column')\n",
    "        if (df['MISC_METADATA'] != '').any():\n",
    "            logging.warning('MISC_METADATA has some data filled in - note this column will be removed from output')\n",
    "        df = df.drop(columns=['MISC_METADATA'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# add_sts_cols(df, contrib_df, gal='Sanger Institute');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sts_manifest(df, input_fn, validation_version):\n",
    "    \n",
    "    output_fn = input_fn.rstrip('.xlsx') + '_' + validation_version + '_for_sts.xlsx'\n",
    "        \n",
    "    logging.info(f'writing STS manifest to \"{output_fn}\"')\n",
    "    \n",
    "    df.to_excel(output_fn, sheet_name='Metadata Entry')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
