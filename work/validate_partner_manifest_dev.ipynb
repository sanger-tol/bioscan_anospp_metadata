{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import itertools\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] test\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# logging.getLogger().setFormat('[%(levelname)s] %(message)s')\n",
    "\n",
    "\n",
    "def setup_logging(verbose=False):\n",
    "    try: \n",
    "        del logging.root.handlers[:]\n",
    "    except:\n",
    "        pass\n",
    "    if verbose:\n",
    "        logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format='[%(levelname)s] %(message)s')\n",
    "setup_logging(verbose=True)   \n",
    "logging.info('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'data/Mike Ashworth NE 2021-06-24 BIOSCAN_Manifest_V1.0.xlsx'\n",
    "template_fn = '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n",
    "# spp_fn = '../../analysis/0_partner/data/harbach_spp_201910.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump ncbi taxonomy\n",
    "today = date.today()\n",
    "taxdump = f'data/taxdump.{today}.tar.gz'\n",
    "if not os.path.isfile(taxdump):\n",
    "    ! wget ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz -O {taxdump}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ete3\n",
    "# download and install taxonomy\n",
    "# ncbi = NCBITaxa()\n",
    "# only run update if needed\n",
    "# ncbi.update_taxonomy_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from 'data/Mike Ashworth NE 2021-06-24 BIOSCAN_Manifest_V1.0.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_data(fn):\n",
    "\n",
    "    logging.info('reading data from {!r}'.format(fn))\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='TAB1 Specimen Metadata Entry')\n",
    "    except:\n",
    "        df = pd.read_excel(fn, dtype=str, index_col=0, keep_default_na=False,\n",
    "                           sheet_name='Specimen Metadata Entry')\n",
    "    \n",
    "    if df.index.duplicated().any():\n",
    "        logging.error('duplicate SERIES: {}'.format(df.index[df.index.duplicated()].to_list()))\n",
    "        \n",
    "    # trailing spaces\n",
    "    for col in df.columns:\n",
    "        trailing_spaces = (df[col].str.startswith(' ') | df[col].str.endswith(' '))\n",
    "        if trailing_spaces.any():\n",
    "            logging.warning('trailing spaces found in {!r}, removing for validation'.format(col,\n",
    "                df.loc[trailing_spaces, col].to_list()))\n",
    "            df[col] = df[col].str.strip()\n",
    "        \n",
    "    return df\n",
    "df = get_data(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] reading data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n"
     ]
    }
   ],
   "source": [
    "template_df = get_data(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] checking manifest columns against template\n"
     ]
    }
   ],
   "source": [
    "def check_columns(df, template_df):\n",
    "    \n",
    "    logging.info('checking manifest columns against template')\n",
    "    \n",
    "    data_cols = set(df.columns)\n",
    "    template_cols = set(template_df.columns)\n",
    "        \n",
    "    if data_cols - template_cols != set():\n",
    "        logging.warning('extra columns in filled manifest compared to template: {}'.format(data_cols - template_cols))\n",
    "    if template_cols - data_cols != set():\n",
    "        logging.error('template columns missing from filled manifest: {}'.format(template_cols - data_cols))\n",
    "check_columns(df, template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Checking and excluding blank samples\n",
      "[INFO] found 10 blank samples based on SCIENTIFIC_NAME\n",
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['CATCH_LOT', 'BOTTLE_DIRECTION', 'WHAT_3_WORDS', 'PHOTOGRAPH_OF_SITE', 'PHOTOGRAPH_OF_CATCH', 'PHOTOGRAPH_OF_PLATE', 'VOUCHER_ID', 'PRESERVATION_APPROACH', 'DATE_OF_PRESERVATION', 'COLLECTOR_SAMPLE_ID', 'ELEVATION', 'OTHER_INFORMATION', 'MISC_METADATA', 'IDENTIFIED_BY', 'IDENTIFIER_AFFILIATION', 'IDENTIFIED_HOW']\n",
      "[INFO] 950 samples of 960 left for downstream analysis\n"
     ]
    }
   ],
   "source": [
    "def check_exclude_blanks(df):\n",
    "    \n",
    "    logging.info('Checking and excluding blank samples')\n",
    "    \n",
    "    # last well of plate expected to be blank\n",
    "    last_well = df[df['TUBE_OR_WELL_ID'] == 'H12']\n",
    "    last_well_blanks = (last_well['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    if not last_well_blanks.all():\n",
    "        logging.error('last well H12 is not blank at SERIES {}'.format(\n",
    "                        last_well[~last_well_blanks].index.to_list()))\n",
    "    \n",
    "    is_blank = (df['SCIENTIFIC_NAME'] == 'blank sample')\n",
    "    blanks = df[is_blank]\n",
    "    \n",
    "    logging.info('found {} blank samples based on SCIENTIFIC_NAME'.format(blanks.shape[0]))\n",
    "    \n",
    "    # check organism part\n",
    "    organism_part_pass = (blanks['ORGANISM_PART'] == 'BLANK_SAMPLE')\n",
    "    if not organism_part_pass.all():\n",
    "        logging.error('for blanks, ORGANISM_PART expected to be BLANK_SAMPLE, found {}'.format(\n",
    "                set(blanks.loc[~organism_part_pass, 'ORGANISM_PART'])))\n",
    "    \n",
    "    # check that NOT_APPLICABLE is filled in all columns\n",
    "    blanks_na = blanks.drop(columns=['ORGANISM_PART','SCIENTIFIC_NAME',\n",
    "                                     'TUBE_OR_WELL_ID','RACK_OR_PLATE_ID',\n",
    "                                     'PRESERVATIVE_SOLUTION'])\n",
    "    na_filled = (blanks_na == 'NOT_APPLICABLE').all(axis=0)\n",
    "    if not na_filled.all():\n",
    "        logging.warning('for blanks, NOT_APPLICABLE expected, but not found in columns {}'.format(\n",
    "                            na_filled[~na_filled].index.to_list()))\n",
    "    # exclude blanks from downstream analysis\n",
    "    \n",
    "    df_flt = df[~is_blank]\n",
    "    \n",
    "    logging.info('{} samples of {} left for downstream analysis'.format(df_flt.shape[0], df.shape[0]))\n",
    "    \n",
    "    return df_flt\n",
    "        \n",
    "df = check_exclude_blanks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] extracting value validation data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n"
     ]
    }
   ],
   "source": [
    "def get_valid_dict(fn):\n",
    "    # pick up validation values from data validation sheet\n",
    "    logging.info('extracting value validation data from {!r}'.format(fn))\n",
    "    valid_df = pd.read_excel(fn, dtype=str, sheet_name='Data Validation - do not edit')\n",
    "    valid_dict = dict()\n",
    "    for col in valid_df.columns:\n",
    "        valid_dict[col] = valid_df[col].dropna().to_list()\n",
    "        \n",
    "    # add 96-well plate well IDs to validation\n",
    "    row_id = list('ABCDEFGH')\n",
    "    col_id = range(1,13)\n",
    "    valid_dict['TUBE_OR_WELL_ID'] = [r + str(c) for (r,c) in itertools.product(row_id, col_id)]\n",
    "    \n",
    "    return valid_dict\n",
    "valid_dict = get_valid_dict(template_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] excluding 665 ['NOT_COLLECTED', ''] samples without data in 'TIME_OF_COLLECTION'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "1      10:48:00\n",
       "2      10:48:00\n",
       "3      10:48:00\n",
       "4      10:48:00\n",
       "5      10:48:00\n",
       "         ...   \n",
       "283    10:48:00\n",
       "284    10:48:00\n",
       "285    10:48:00\n",
       "286    10:48:00\n",
       "287    10:48:00\n",
       "Name: TIME_OF_COLLECTION, Length: 285, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exclude_missing(series, na_values=None):\n",
    "    \n",
    "    # valid missing data \n",
    "    no_data = (series.isin(na_values))\n",
    "    if no_data.sum() > 0:\n",
    "        logging.info('excluding {} {!r} samples without data in {!r}'.format(no_data.sum(), na_values, series.name))\n",
    "    return series[~no_data]\n",
    "    \n",
    "exclude_missing(df['TIME_OF_COLLECTION'], na_values=['NOT_COLLECTED', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating values in column 'ORGANISM_PART'\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n"
     ]
    }
   ],
   "source": [
    "def validate_values(col, df, valid_dict, sep=None, na_values=None, level='e'):\n",
    "    \n",
    "    logging.info('validating values in column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    if col not in valid_dict.keys():\n",
    "        logging.error('{!r} column not found in validation sheet'.format(col))\n",
    "        return\n",
    "    assert level in ('i','w','e'), '{!r} invalid logging level for validate_values'.format(level)\n",
    "    \n",
    "    series = df[col]\n",
    "    if na_values:\n",
    "        series = exclude_missing(series, na_values)\n",
    "    \n",
    "    col_values = set(series.unique())\n",
    "    # use separator to split values\n",
    "    if sep:\n",
    "        sep_col_values = list()\n",
    "        for v in col_values:\n",
    "            sep_col_values.extend([x.strip() for x in v.split(sep)])\n",
    "        col_values = set(sep_col_values)\n",
    "    valid_values = set(valid_dict[col])\n",
    "    invalid_values = col_values - valid_values\n",
    "    if len(invalid_values) > 0:\n",
    "        msg = 'invalid values in {!r}: {}'.format(col, invalid_values)\n",
    "        if level == 'i':\n",
    "            logging.info(msg)\n",
    "        elif level == 'w':\n",
    "            logging.warning(msg)\n",
    "        elif level == 'e':\n",
    "            logging.error(msg)\n",
    "#     else:\n",
    "#         logging.info('all values valid in {!r}'.format(col))\n",
    "            \n",
    "validate_values('ORGANISM_PART', df, valid_dict, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['NOT_COLLECTED' '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "2     2021-06-24\n",
       "3     2021-06-24\n",
       "4     2021-06-24\n",
       "5     2021-06-24\n",
       "6     2021-06-24\n",
       "         ...    \n",
       "283   2021-06-24\n",
       "284   2021-06-24\n",
       "285   2021-06-24\n",
       "286   2021-06-24\n",
       "287   2021-06-24\n",
       "Name: DATE_OF_COLLECTION, Length: 284, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_date(col, df):\n",
    "    \n",
    "    logging.info('validating date column {!r}'.format(col))\n",
    "\n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    # missing date not allowed\n",
    "    # series = exclude_missing(series, na_values)\n",
    "    \n",
    "    # invalid date formats\n",
    "    # empty string converted to NaT\n",
    "    date_series = pd.to_datetime(series, format='%Y-%m-%d', errors='coerce')\n",
    "    if date_series.isna().any():\n",
    "        logging.error('invalid dates in {!r}: {}'.format(col, \n",
    "                                                         series[date_series.isna()].unique()))\n",
    "    valid_date_series = date_series[~date_series.isna()]\n",
    "    \n",
    "    # dates in future\n",
    "    future_dates = (valid_date_series > datetime.datetime.today())\n",
    "    if future_dates.any():\n",
    "        logging.error('future dates in {!r}: {}'.format(col,\n",
    "            valid_date_series[future_dates].to_list()))\n",
    "        \n",
    "    # dates too old\n",
    "    old_dates = (valid_date_series < datetime.datetime.strptime('1900-01-01', '%Y-%m-%d'))\n",
    "    if old_dates.any():\n",
    "        logging.error(\"pre-1900 dates in {!r}: {}\".format(col,\n",
    "            valid_date_series[old_dates].to_list())) \n",
    "    \n",
    "    return valid_date_series\n",
    "df.loc[1,'DATE_OF_COLLECTION'] = 'NOT_COLLECTED'\n",
    "validate_date('DATE_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SERIES\n",
       "1     1900-01-01 10:48:00\n",
       "2     1900-01-01 10:48:00\n",
       "3     1900-01-01 10:48:00\n",
       "4     1900-01-01 10:48:00\n",
       "5     1900-01-01 10:48:00\n",
       "              ...        \n",
       "283   1900-01-01 10:48:00\n",
       "284   1900-01-01 10:48:00\n",
       "285   1900-01-01 10:48:00\n",
       "286   1900-01-01 10:48:00\n",
       "287   1900-01-01 10:48:00\n",
       "Name: TIME_OF_COLLECTION, Length: 285, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_time(col, df, na_values=['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating time column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "        \n",
    "    # invalid time formats\n",
    "    # NB empty string converted to NaT\n",
    "    time_series = pd.to_datetime(series, format='%H:%M:%S', errors='coerce')\n",
    "    if time_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "                                                         series[time_series.isna()].unique()))\n",
    "    valid_time_series = time_series[~time_series.isna()]\n",
    "    \n",
    "    return valid_time_series\n",
    "# df.loc[1,'TIME_OF_COLLECTION'] = '23'\n",
    "validate_time('TIME_OF_COLLECTION', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n"
     ]
    }
   ],
   "source": [
    "def validate_time_period(col, df, na_values=['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating time period column {!r}'.format(col))\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "\n",
    "    # conversion with modifications for proper parsing \n",
    "    # by pd.Timedelta (does not accept missing data, e.g. 'PT1H')\n",
    "    # note - will not work for weeks and months\n",
    "    def convert_iso_duration(s):\n",
    "        if s == np.nan:\n",
    "            return np.nan\n",
    "        if not s.startswith('P') or 'T' not in s:\n",
    "            return np.nan\n",
    "        # add days\n",
    "        if s.startswith('PT'):\n",
    "            s = s.replace('PT','P0DT')\n",
    "        # add trailing minutes and seconds\n",
    "        if s.endswith('H'):\n",
    "            s += '0M0S'\n",
    "        elif s.endswith('M'):\n",
    "            s += '0S'\n",
    "        try:\n",
    "            return pd.Timedelta(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    time_period_series = series.apply(convert_iso_duration)\n",
    "    if time_period_series.isna().any():\n",
    "        logging.error('invalid times in {!r}: {}'.format(col, \n",
    "            series[time_period_series.isna()].unique()))\n",
    "    valid_time_period_series = time_period_series[~time_period_series.isna()]\n",
    "    return valid_time_period_series\n",
    "\n",
    "# df.loc[1,'DURATION_OF_COLLECTION'] = 'PVT1H'\n",
    "validate_time_period('DURATION_OF_COLLECTION', df);\n",
    "# df['DURATION_OF_COLLECTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] validating country with coordinates\n",
      "[WARNING] could not locate country for coordinates ', ', partner country ''\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COLLECTION_LOCATION</th>\n",
       "      <th>DECIMAL_LATITUDE</th>\n",
       "      <th>DECIMAL_LONGITUDE</th>\n",
       "      <th>coord</th>\n",
       "      <th>partner_country</th>\n",
       "      <th>coord_country</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SERIES</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United Kingdom | England | Shapwick NNR | Roug...</td>\n",
       "      <td>51.161693</td>\n",
       "      <td>-2.8149448</td>\n",
       "      <td>51.161693, -2.8149448</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United Kingdom | England | Shapwick NNR | Roug...</td>\n",
       "      <td>51.161693</td>\n",
       "      <td>-2.8149448</td>\n",
       "      <td>51.161693, -2.8149448</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United Kingdom | England | Shapwick NNR | Roug...</td>\n",
       "      <td>51.161693</td>\n",
       "      <td>-2.8149448</td>\n",
       "      <td>51.161693, -2.8149448</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United Kingdom | England | Shapwick NNR | Roug...</td>\n",
       "      <td>51.161693</td>\n",
       "      <td>-2.8149448</td>\n",
       "      <td>51.161693, -2.8149448</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>United Kingdom | England | Shapwick NNR | Roug...</td>\n",
       "      <td>51.161693</td>\n",
       "      <td>-2.8149448</td>\n",
       "      <td>51.161693, -2.8149448</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "      <td>UNITED KINGDOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COLLECTION_LOCATION DECIMAL_LATITUDE  \\\n",
       "SERIES                                                                       \n",
       "1       United Kingdom | England | Shapwick NNR | Roug...        51.161693   \n",
       "2       United Kingdom | England | Shapwick NNR | Roug...        51.161693   \n",
       "3       United Kingdom | England | Shapwick NNR | Roug...        51.161693   \n",
       "4       United Kingdom | England | Shapwick NNR | Roug...        51.161693   \n",
       "5       United Kingdom | England | Shapwick NNR | Roug...        51.161693   \n",
       "...                                                   ...              ...   \n",
       "955                                                                          \n",
       "956                                                                          \n",
       "957                                                                          \n",
       "958                                                                          \n",
       "959                                                                          \n",
       "\n",
       "       DECIMAL_LONGITUDE                  coord partner_country  \\\n",
       "SERIES                                                            \n",
       "1             -2.8149448  51.161693, -2.8149448  UNITED KINGDOM   \n",
       "2             -2.8149448  51.161693, -2.8149448  UNITED KINGDOM   \n",
       "3             -2.8149448  51.161693, -2.8149448  UNITED KINGDOM   \n",
       "4             -2.8149448  51.161693, -2.8149448  UNITED KINGDOM   \n",
       "5             -2.8149448  51.161693, -2.8149448  UNITED KINGDOM   \n",
       "...                  ...                    ...             ...   \n",
       "955                                          ,                    \n",
       "956                                          ,                    \n",
       "957                                          ,                    \n",
       "958                                          ,                    \n",
       "959                                          ,                    \n",
       "\n",
       "         coord_country  \n",
       "SERIES                  \n",
       "1       UNITED KINGDOM  \n",
       "2       UNITED KINGDOM  \n",
       "3       UNITED KINGDOM  \n",
       "4       UNITED KINGDOM  \n",
       "5       UNITED KINGDOM  \n",
       "...                ...  \n",
       "955            UNKNOWN  \n",
       "956            UNKNOWN  \n",
       "957            UNKNOWN  \n",
       "958            UNKNOWN  \n",
       "959            UNKNOWN  \n",
       "\n",
       "[950 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to be replaced/supported by w3w check\n",
    "def check_location(df, fn):\n",
    "    \n",
    "    logging.info('validating country with coordinates')\n",
    "    \n",
    "    loc_col, lat_col, lon_col = 'COLLECTION_LOCATION', 'DECIMAL_LATITUDE', 'DECIMAL_LONGITUDE'\n",
    "\n",
    "    try:\n",
    "        loc_df_complete = df[[loc_col, lat_col, lon_col]].copy()\n",
    "    except:\n",
    "        logging.error('One of {!r} {!r} {!r} columns not found in manifest'.format(loc_col, lat_col, lon_col))\n",
    "        return\n",
    "#     loc_df_isna = (loc_df.isin(na_values)).all(axis=1)\n",
    "#     if loc_df_isna.any():\n",
    "#         logging.info('removing {} {!r} samples with missing data from coordinate analysis'.format(\n",
    "#                 loc_df_isna.sum(), na_values))\n",
    "#     loc_df_complete = loc_df[~loc_df_isna].copy()\n",
    "    \n",
    "    # coordinates in geopy format\n",
    "    loc_df_complete['coord'] = loc_df_complete.apply(lambda x: '{}, {}'.format(\n",
    "            x[lat_col], x[lon_col]), axis=1)\n",
    "    \n",
    "    # get location data for coordinates\n",
    "    # use local copy of web query results for re-runs\n",
    "    # this \n",
    "    loc_fn = fn+'_loc.pkl'\n",
    "    if os.path.isfile(loc_fn):\n",
    "        locations = pickle.load(open(loc_fn, \"rb\"))\n",
    "    else:\n",
    "        # web map server - openstreetmaps\n",
    "        logging.info('querying coordinates')\n",
    "        locator = Nominatim(user_agent='myGeocoder')\n",
    "        rgeocode = RateLimiter(locator.reverse, min_delay_seconds=1)\n",
    "\n",
    "        locations = dict()\n",
    "        for c in loc_df_complete.coord.unique():\n",
    "            # pre-fill with unknown country\n",
    "            locations[c] = {'address':{'country':'UNKNOWN'}}\n",
    "            # check coordniate correctness\n",
    "            try:\n",
    "                lat, lon = c.split(', ')\n",
    "                lat, lon = float(lat), float(lon)\n",
    "            except:\n",
    "                logging.error('problem parsing coordinates {!r}'.format(c))\n",
    "                continue\n",
    "            if abs(lat) > 90:\n",
    "                logging.error('invalid latitude {}, should be in [-90,90]'.format(lat))\n",
    "                continue\n",
    "            if abs(lon) > 180:\n",
    "                logging.error('invalid longitude {}, should be in [-180,180]'.format(lon))\n",
    "                continue\n",
    "            # web query\n",
    "            location = rgeocode(c, language='en-gb')\n",
    "            # rgeocode returns empty location outside of counries and in some other situations\n",
    "            if location is not None:\n",
    "                locations[c] = location.raw\n",
    "\n",
    "        # save locations to file\n",
    "        pickle.dump(locations, open(loc_fn, \"wb\"))\n",
    "        \n",
    "    # parse country from partner input\n",
    "    loc_df_complete['partner_country'] = loc_df_complete[loc_col].apply(lambda x: x.split('|')[0].strip().upper())\n",
    "    \n",
    "    # extract countries from location data\n",
    "    loc_countries = dict()\n",
    "    for coord in locations.keys():\n",
    "        coord_country = locations[coord]['address']['country'].upper()\n",
    "        loc_countries[coord] = coord_country\n",
    "        \n",
    "        partner_countries = loc_df_complete.loc[loc_df_complete.coord == coord, 'partner_country']\n",
    "        if partner_countries.nunique() > 1:\n",
    "            logging.error('multiple partner countries for coordinates {!r}: {}'\n",
    "                          'skipping coordinate validation'.format(\n",
    "                                coord, partner_countries.unique()))\n",
    "            continue\n",
    "        if partner_countries.shape[0] == 0:\n",
    "            logging.error('no partner location found for coordinates {!r}'.format(coord))\n",
    "            continue\n",
    "        partner_country = partner_countries.iloc[0]\n",
    "        if coord_country == 'UNKNOWN':\n",
    "            logging.warning('could not locate country for coordinates {!r}, partner country {!r}'.format(\n",
    "                    coord, partner_country))\n",
    "        elif partner_country != coord_country:\n",
    "            logging.error('country mismatch for coordinates {!r}, partner country {!r}, '\n",
    "                          'coordinate country {!r}'.format(coord, partner_country, coord_country))\n",
    "    \n",
    "    # countries based on coordinates\n",
    "    loc_df_complete['coord_country'] = loc_df_complete['coord'].replace(loc_countries)\n",
    "    country_mismatch = (loc_df_complete.coord_country != loc_df_complete.partner_country)\n",
    "\n",
    "#     if country_mismatch.any():\n",
    "#         logging.error('coordinates do not match country for SERIES: {}'.format(\n",
    "#                 country_mismatch[country_mismatch].index.to_list()))\n",
    "    \n",
    "    # location data can be re-used, e.g. as an additional field\n",
    "    return loc_df_complete\n",
    "# df.loc[2,'DECIMAL_LATITUDE'] = '65'\n",
    "loc_test = check_location(df, fn)\n",
    "loc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = pd.DataFrame(loc_dict).T\n",
    "# pd.DataFrame(l['address'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# valid_spp = list()\n",
    "# valid_spp = pd.read_csv(spp_fn, header=None)[0].to_list()\n",
    "# len(valid_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be replaced with ete3 taxonomy lookup\n",
    "def validate_scientific_names(col, df, spp_fn, na_values = ['NOT_COLLECTED']):\n",
    "    \n",
    "    logging.info('validating species names against {!r}'.format(spp_fn))\n",
    "\n",
    "    # read species names from file\n",
    "    valid_spp = pd.read_csv(spp_fn, header=None)[0].to_list()\n",
    "    \n",
    "    if col not in df.columns:\n",
    "        logging.error('{!r} column not found in manifest'.format(col))\n",
    "        return\n",
    "    series = df[col]\n",
    "    series = exclude_missing(series, na_values)\n",
    "    \n",
    "    correct_genus = series.str.startswith('Anopheles')\n",
    "    if not correct_genus.all():\n",
    "        logging.error('expected Anopheles as genus in {!r} column, found: {}'.format(\n",
    "                      col, series[~correct_genus].unique()))\n",
    "    \n",
    "    species = series.str.split(' ').str.get(1)\n",
    "    correct_species = species.isin(valid_spp)\n",
    "    \n",
    "    if not correct_species.all():\n",
    "        logging.error('species not in {!r} found in {!r} column: {}'.format(\n",
    "                      spp_fn, col, series[~correct_species].unique()))\n",
    "# df.loc[1, 'SCIENTIFIC_NAME'] = 'Anapherla askfnvjn'\n",
    "# validate_scientific_names('SCIENTIFIC_NAME', df, spp_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_undeleted_example(df):\n",
    "    \n",
    "    logging.info('checking if example row was not deleted')\n",
    "    \n",
    "    if (df.index == 'malaise_example-small').any():\n",
    "        logging.error('example index was not deleted')\n",
    "    elif (df.RACK_OR_PLATE_ID == 'NBGW-001').any():\n",
    "        logging.error('example plate ID was not deleted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] # started validate_partner_manifest_v.1.0\n",
      "[INFO] reading data from 'data/Mike Ashworth NE 2021-06-24 BIOSCAN_Manifest_V1.0.xlsx'\n",
      "[INFO] reading data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n",
      "[INFO] checking manifest columns against template\n",
      "[INFO] extracting value validation data from '../data/BIOSCAN_Manifest_V1.0_20211207.xlsx'\n",
      "[INFO] Checking and excluding blank samples\n",
      "[INFO] found 10 blank samples based on SCIENTIFIC_NAME\n",
      "[WARNING] for blanks, NOT_APPLICABLE expected, but not found in columns ['CATCH_LOT', 'BOTTLE_DIRECTION', 'WHAT_3_WORDS', 'PHOTOGRAPH_OF_SITE', 'PHOTOGRAPH_OF_CATCH', 'PHOTOGRAPH_OF_PLATE', 'VOUCHER_ID', 'PRESERVATION_APPROACH', 'DATE_OF_PRESERVATION', 'COLLECTOR_SAMPLE_ID', 'ELEVATION', 'OTHER_INFORMATION', 'MISC_METADATA', 'IDENTIFIED_BY', 'IDENTIFIER_AFFILIATION', 'IDENTIFIED_HOW']\n",
      "[INFO] 950 samples of 960 left for downstream analysis\n",
      "[INFO] validating values in column 'PRESERVATIVE_SOLUTION'\n",
      "[ERROR] invalid values in 'PRESERVATIVE_SOLUTION': {''}\n",
      "[INFO] validating values in column 'TUBE_OR_WELL_ID'\n",
      "[ERROR] invalid values in 'TUBE_OR_WELL_ID': {''}\n",
      "[INFO] validating values in column 'BOTTLE_DIRECTION'\n",
      "[ERROR] invalid values in 'BOTTLE_DIRECTION': {''}\n",
      "[INFO] validating values in column 'ORGANISM_PART'\n",
      "[ERROR] invalid values in 'ORGANISM_PART': {''}\n",
      "[INFO] validating values in column 'HAZARD_GROUP'\n",
      "[ERROR] invalid values in 'HAZARD_GROUP': {''}\n",
      "[INFO] validating values in column 'REGULATORY_COMPLIANCE'\n",
      "[ERROR] invalid values in 'REGULATORY_COMPLIANCE': {''}\n",
      "[INFO] validating date column 'DATE_OF_COLLECTION'\n",
      "[ERROR] invalid dates in 'DATE_OF_COLLECTION': ['']\n",
      "[INFO] validating country with coordinates\n",
      "[WARNING] could not locate country for coordinates ', ', partner country ''\n",
      "[INFO] validating values in column 'LIFESTAGE'\n",
      "[ERROR] invalid values in 'LIFESTAGE': {''}\n",
      "[INFO] validating values in column 'SEX'\n",
      "[ERROR] invalid values in 'SEX': {''}\n",
      "[INFO] validating time column 'TIME_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'TIME_OF_COLLECTION': ['']\n",
      "[INFO] validating time period column 'DURATION_OF_COLLECTION'\n",
      "[ERROR] invalid times in 'DURATION_OF_COLLECTION': ['']\n",
      "[INFO] validating values in column 'COLLECTION_METHOD'\n",
      "[ERROR] invalid values in 'COLLECTION_METHOD': {''}\n",
      "[INFO] validating date column 'DATE_OF_PRESERVATION'\n",
      "[ERROR] invalid dates in 'DATE_OF_PRESERVATION': ['']\n",
      "[INFO] checking if example row was not deleted\n",
      "/Users/am60/miniconda3/envs/validate_partner_manifest_dev/lib/python3.7/site-packages/pandas/core/indexes/base.py:140: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = op(self._values, np.asarray(other))\n",
      "[INFO] # ended validate_partner_manifest_v.1.0\n"
     ]
    }
   ],
   "source": [
    "def validate(fn, template_fn, verbose=False, version='1.0'):\n",
    "    '''\n",
    "    Validation follows the order of columns order in data entry sheet\n",
    "    '''\n",
    "\n",
    "    setup_logging(verbose=verbose)\n",
    "\n",
    "    logging.info('# started validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    # read data\n",
    "    df = get_data(fn)\n",
    "    \n",
    "    # prepare validation\n",
    "    template_df = get_data(template_fn)\n",
    "    check_columns(df, template_df)\n",
    "    valid_dict = get_valid_dict(template_fn)\n",
    "\n",
    "    # exclude blanks\n",
    "    df = check_exclude_blanks(df)\n",
    "    \n",
    "    #orange cols\n",
    "    # BOTTLE_DIRECTION not checked TODO do not allow missing\n",
    "    validate_values('PRESERVATIVE_SOLUTION', df, valid_dict)\n",
    "    validate_values('TUBE_OR_WELL_ID', df, valid_dict)\n",
    "    # CATCH_LOT not checked TODO do not allow missing\n",
    "    validate_values('BOTTLE_DIRECTION', df, valid_dict)\n",
    "    validate_values('ORGANISM_PART', df, valid_dict, sep='|')\n",
    "    validate_values('HAZARD_GROUP', df, valid_dict)\n",
    "    validate_values('REGULATORY_COMPLIANCE', df, valid_dict)\n",
    "    validate_date('DATE_OF_COLLECTION', df)\n",
    "    check_location(df, fn)\n",
    "    \n",
    "    # purple cols\n",
    "    # validate_scientific_names('SCIENTIFIC_NAME', df, spp_fn, na_values=['NOT_COLLECTED'])\n",
    "    validate_values('LIFESTAGE', df, valid_dict)\n",
    "    validate_values('SEX', df, valid_dict)\n",
    "    # HABITAT not checked\n",
    "    validate_time('TIME_OF_COLLECTION', df)\n",
    "    validate_time_period('DURATION_OF_COLLECTION', df)\n",
    "    validate_values('COLLECTION_METHOD', df, valid_dict)\n",
    "    # DESCRIPTION_OF_COLLECTION_METHOD not checked\n",
    "    # TODO validate_format('TIME_ELAPSED_FROM_COLLECTION_TO_PRESERVATION', dtype=int)\n",
    "    # PHOTOGRAPH_* columns not checked\n",
    "    # VOUCHER_ID not checked\n",
    "    # PRESERVATION_APPROACH not checked - should match DATE_OF_PRESERVATION\n",
    "    validate_date('DATE_OF_PRESERVATION', df) # allow for empty values unlike DATE_OF_COLLECTION\n",
    "    # TODO compare_dates(before='DATE_OF_COLLECTION', after='DATE_OF_PRESERVATION')\n",
    "    # COLLECTOR_SAMPLE_ID not checked\n",
    "    # TODO validate_format('ELEVATION', dtype=int)\n",
    "    # OTHER_INFORMATION\tMISC_METADATA\tIDENTIFIED_BY\tIDENTIFIER_AFFILIATION\tIDENTIFIED_HOW not checked\n",
    "    \n",
    "    check_undeleted_example(df)\n",
    "    \n",
    "    logging.info('# ended validate_partner_manifest_v.{}'.format(version))\n",
    "\n",
    "    return df\n",
    "\n",
    "# fn = '../../results/partner_manifests/IRD-Neandersquito_T222Amplicon_Manifest_V2.0.xlsx'\n",
    "df = validate(fn, template_fn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
